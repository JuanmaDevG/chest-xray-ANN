# -*- coding: utf-8 -*-
"""chest_xray_LeNet5_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EfaOHNpkh9HHdjqPSGclsu1O3rOhzk7K

# Chest X-Ray dataset with LeNet 5

## Chest X-Ray

Methods to diagnose pneumonia using chest x-ray images (<https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia>)

In this case, the purpose is to classify the level of pneumonia in a given image.

Trains a simple LeNet-5 (http://yann.lecun.com/exdb/lenet/) adapted to the `Chest X-Ray` dataset using Keras Software (http://keras.io/)

LeNet-5 demo example http://eblearn.sourceforge.net/beginner_tutorial2_train.html

## Local instalation

### Install the following Python packages to run this notebook

`pip install pip -U`

`pip install keras tensorflow pillow h5py sklearn jupyter`

## Google Colab

[Google Colab](https://colab.research.google.com/) is a research project created to help disseminate machine learning education and research. It's a `Jupyter notebook` environment that requires no setup to use and runs entirely in the cloud.

Colaboratory notebooks are stored in [Google Drive](https://drive.google.com) and can be shared just as you would with Google Docs or Sheets. Colaboratory is free to use.

For more information, see our [FAQ](https://research.google.com/colaboratory/faq.html).

### How install extra packages
Google Colab installs a series of basic packages if we need any additional package just install it.
"""

!pip install -q scikit-learn -U

"""## Copy dataset to Colab"""

!wget https://www.dlsi.ua.es/~juanra/UA/datasets/chest_xray_512.zip

"""If we use an uploaded compress file we can uncompress with:"""

!unzip -q chest_xray_512.zip
!ls

"""## Import packages"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import glob

import sklearn
from sklearn.model_selection import train_test_split
from sklearn import metrics

from tensorflow import keras
from tensorflow.keras.preprocessing import image
from tensorflow.keras import models
from tensorflow.keras import layers

"""## Define global constants

Lets start with 50 epochs to test learning network parameters
"""

batch_size = 64
nb_classes = 2
epochs = 50

# Scaling input image to theses dimensions
img_rows, img_cols = 32, 32

"""## Load image database"""

#
#	Load data from data/chest_xray_512/ NORMAL (0) PNEUMONIA(1)
#

def load_data():
  name_classes = ['NORMAL','PNEUMONIA']
  X,y  = [], []
  for class_number, class_name in enumerate(name_classes):    # Number of directories
    for filename in glob.glob(f'./chest_xray_512/{class_name}/*.jpg'):
      im = image.load_img(filename, target_size=[img_rows, img_cols], color_mode = 'grayscale')
      X.append(image.img_to_array(im))
      y.append(class_number)

  input_shape = (img_rows, img_cols, 1)

  return np.array(X), np.array(y), input_shape

"""## Plot images"""

def plot_symbols(X,y,n=15):
    index = np.random.randint(len(y), size=n)
    plt.figure(figsize=(n, 3))
    for i in np.arange(n):
        ax = plt.subplot(1,n,i+1)
        plt.imshow(X[index[i],:,:,0])
        plt.gray()
        ax.set_title('{}-{}'.format(y[index[i]],index[i]))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()

"""## Build LeNet5 structure

<center><img src="https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/LeNet5.jpg"></center>
"""

#
# Build an ANN structure - LeNet5
#

def cnn_model(input_shape):
    #
    # Neural Network Structure
    #

    model = models.Sequential()

    model.add(layers.Input(shape=input_shape))
    model.add(layers.Rescaling(1./255))

    model.add(layers.Conv2D(6, (5, 5)))
    model.add(layers.Activation("sigmoid"))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))

    model.add(layers.Conv2D(16, (5, 5)))
    model.add(layers.Activation("sigmoid"))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))

    model.add(layers.Flatten())

    model.add(layers.Dense(120))
    model.add(layers.Activation("sigmoid"))

    model.add(layers.Dense(84))
    model.add(layers.Activation("sigmoid"))

    model.add(layers.Dense(nb_classes))
    model.add(layers.Activation('softmax'))

    return model

"""## Start to run the program

### Load data
"""

##################################################################################
# Main program

X, y, input_shape = load_data()

print(X.shape, 'train samples')
print(img_rows,'x', img_cols, 'image size')
print(input_shape,'input_shape')
print(epochs,'epochs')

"""### Let to see examples of the dataset"""

plot_symbols(X, y)

"""## Number of examples per class"""

import collections

collections.Counter(y)

"""## Split examples in training/test sets"""

# CNN layer need an additional chanel to colors (32 x 32 x 1)
print('N samples, witdh, height, channels',X.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)

print(f'x_train {X_train.shape} x_test {X_test.shape}')
print(f'y_train {y_train.shape} y_test {y_test.shape}')

"""### Model and optimizers

Test [optimizer](https://keras.io/optimizers/) parameter with `sgd`, `adadelta` or `adam` values in order to check the final precision achieved.
"""

model = cnn_model(input_shape)
print(model.summary())

model.compile(loss='sparse_categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])

#early_stopping = EarlyStopping(monitor='val_loss', patience=10)
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])

"""> **Questions**:
 - What is `sparse_categorical_crossentropy` for?
 - And if we use `categorical_crossentropy`, what good would it do?
"""

loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)
print(f'loss: {loss:.2f} acc: {acc:.2f}')

"""## Prediction

### Testing AUC result for two classes
"""

from sklearn.metrics import roc_auc_score

#
# Results AUC (Area Under ROC)
#

y_pred = model.predict(X_test) #Extract prediction per sample and class
print(f'AUC {roc_auc_score(y_test, y_pred[:,1], ):.4f}')

"""**Note**: If multiple classes (>2) are used, the parameter 'multi_class' = 'ovr' (over-vs-rest) should be used.

## More metrics about results

We can find more information about `precision`, `recall` and `f1` metrics in <https://en.wikipedia.org/wiki/Precision_and_recall>.
"""

import matplotlib.pyplot as plt

print('Predictions')
y_pred_int = y_pred.argmax(axis=1)
print(collections.Counter(y_pred_int),'\n')

print('Metrics')
print(metrics.classification_report(y_test, y_pred_int, target_names=['Normal','Pneumonia']))

print('Confusion matrix')
metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(y_test,y_pred_int), display_labels=['NORMAL','PNEUMONIA']).plot()
plt.show()